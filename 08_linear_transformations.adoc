= Linear Transformations =
:doctype: book
:author: Bharath Kumar S P
:email: bharath030195@gmail.com
:stem: latexmath
:eqnums:
:toc:

== Introduction ==
Linear algebra is the study of linear functions between vector spaces. Suppose there are two vector spaces stem:[\mathbb{V} = (V, \+, \cdot)] and stem:[\mathbb{W} = (W, +, \cdot)]. The stem:[(+, \cdot)] doesn't need to be the same. Now we can talk about transformations of functions.

Say for a vector stem:[\mathbf{v} \in \mathbb{V}], we apply a transformation stem:[T(\mathbf{v})] and it gives a vector stem:[\mathbf{w} \in \mathbb{W}]. If we define such a rule for all the vectors in stem:[\mathbb{V}], then we can call it as a valid function. Some of the valid functions are

* stem:[T(\mathbf{v}) = \|\mathbf{v}\|], which is a transformation from stem:[\mathbb{R}^n \rightarrow \mathbb{R}] in the case of Euclidean space.
* stem:[T(\mathbf{v}) = \mathbf{v} + \mathbf{w}_0] where stem:[ \mathbf{w}_0] is a constant vector. This is a transformation from a vector space to itself. Transformations which are from a space to itself are called as *operators*.

We are interested in transformations that are linear.

== Linear Transformations ==
Let stem:[\mathbb{V}] and stem:[\mathbb{W}] be vector spaces of dimensions stem:[N] and stem:[M] respectively. A linear transformation is a mapping stem:[T: \mathbb{V} \rightarrow \mathbb{W}] such that

. stem:[T(\mathbf{v}_1 + \mathbf{v}_2) = T(\mathbf{v}_1) + T(\mathbf{v}_2) \hspace{1cm} \forall \mathbf{v}_1, \mathbf{v}_2 \in \mathbb{V} ].
. stem:[T(\alpha \cdot \mathbf{v}) = \alpha \cdot T(\mathbf{v}) \hspace{1cm} \forall \mathbf{v} \in \mathbb{V} ].

Transformation of the linear combination is the same as the linear combination of the transformation,

[stem]
++++
T(\alpha \cdot \mathbf{v}_1 + \beta \cdot \mathbf{v}_2 ) = \alpha \cdot T(\mathbf{v}_1) + \beta \cdot T(\mathbf{v}_2) 
++++

image::.\images\linear_trans_01.png[align='center']

== Examples ==

*Example 01:*

Let's consider the Euclidean vector space, stem:[\mathbb{V} = \mathbb{R}^n] and stem:[\mathbb{W}= \mathbb{R}]. And let stem:[\mathbf{w}_0] be a constant vector in stem:[\mathbb{V}]. Let stem:[T(\mathbf{v}) \equiv \langle \mathbf{w}_0, \mathbf{v} \rangle ]. To prove this is a linear transformation,

[stem]
++++
\begin{align*}
T(\alpha \cdot \mathbf{v}_1 + \beta \cdot \mathbf{v}_2 ) & = \langle \mathbf{w}_0, \alpha \cdot \mathbf{v}_1 + \beta \cdot \mathbf{v}_2 \rangle \\
& = \alpha \langle \mathbf{w}_0, \mathbf{v}_1 \rangle + \beta \langle \mathbf{w}_0, \mathbf{v}_2 \rangle \\
& = \alpha \cdot T(\mathbf{v}_1) + \beta \cdot T(\mathbf{v}_2)
\end{align*}
++++

*Example 02:*

Let's consider the Euclidean vector space, stem:[\mathbb{V} = \mathbb{R}^n] and stem:[\mathbb{W}= \mathbb{R}^2]. And let stem:[\mathbf{w}_1] and stem:[\mathbf{w}_2] be constant vectors in stem:[\mathbb{V}]. Let

[stem]
++++
T(\mathbf{v}) \equiv \begin{bmatrix} \mathbf{w}_1^\top \mathbf{v} \\ \mathbf{w}_2^\top \mathbf{v} \end{bmatrix}
++++

This is a linear transformation as every component is a linear transformation.

[stem]
++++
\begin{align*}
T(\alpha \cdot \mathbf{v}_1 + \beta \cdot \mathbf{v}_2 ) = \begin{bmatrix} \mathbf{w}_1^\top (\alpha \cdot \mathbf{v}_1 + \beta \cdot \mathbf{v}_2 )\\ \mathbf{w}_2^\top (\alpha \cdot \mathbf{v}_1 + \beta \cdot \mathbf{v}_2 ) \end{bmatrix}

& = \begin{bmatrix} \alpha \mathbf{w}_1^\top \mathbf{v}_1 + \beta \mathbf{w}_1^\top \mathbf{v}_2 \\ \alpha \mathbf{w}_2^\top \mathbf{v}_1 + \beta \mathbf{w}_2^\top \mathbf{v}_2 \end{bmatrix} \\
\\
&= \alpha \cdot T(\mathbf{v}_1) + \beta \cdot T(\mathbf{v}_2)
\end{align*}
++++

Let's define a matrix stem:[\mathbf{M} = \begin{bmatrix} \mathbf{w}_1^\top \\ \mathbf{w}_2^\top \end{bmatrix}_{2 \times n}]. Then the above transformation can be written as stem:[T(\mathbf{v}) = \mathbf{M} \mathbf{v}].

== Matrix as Transformations ==
We can generalize this to any transformation. If we want a linear transformation from stem:[\mathbb{R}^n \to \mathbb{R}^m], then we need a matrix stem:[\mathbf{A}_{m \times n}]. Multiplying a vector with a matrix is a linear transformation, ie., the transformation

[stem]
++++
T(\mathbf{v}) = \mathbf{A} \mathbf{v}
++++

is always a linear transformation. But what about the converse? Can we express all the linear transformations in this form? If this result was true, we should be able to prove the following *Representer theorem*.

=== To One Dimension ===
Let stem:[T: \mathbb{R}^n \rightarrow \mathbb{R}] and it is linear. We need to show that there exists stem:[\mathbf{w} \in \mathbb{R}^n] (in the original space itself) such that the transformation can be expressed as stem:[T(\mathbf{v}) = \mathbf{w}^\top \mathbf{v}].

To define a transformation, we need to define it for every stem:[\mathbf{v} \in \mathbb{V}], but by this theorem, we can define the whole transformation by a single vector, i.e., stem:[n] numbers are enough to define this transformation. Such reduction in specification is facilitated by this theorem.

We need to find a vector stem:[\mathbf{w}^\top = \begin{bmatrix} w_1 & w_2 & \dots & w_n \end{bmatrix}].

*Proof:* Let

[stem]
++++
\mathbf{e}_1 = \begin{bmatrix} 1 \\ 0 \\ \vdots \\ 0 \end{bmatrix} \implies T\left( \begin{bmatrix} 1 \\ 0 \\ \vdots \\ 0 \end{bmatrix} \right) = \begin{bmatrix} w_1 & w_2 & \dots & w_n \end{bmatrix} \begin{bmatrix} 1 \\ 0 \\ \vdots \\ 0 \end{bmatrix} = w_1 \implies w_1 = T(\mathbf{e}_1)
++++

If stem:[\mathbf{e}_2 = \begin{bmatrix} 0 \\ 1 \\ \vdots \\ 0 \end{bmatrix}], then we get stem:[w_2 = T(\mathbf{e}_2)]. Similarly, stem:[w_i = T(\mathbf{e}_i)] for stem:[i=3,\dots, n].

If the theorem is true, the only choice for stem:[\mathbf{w}] will be stem:[\begin{bmatrix} T(\mathbf{e}_1) \\ T(\mathbf{e}_2) \\ \vdots \\ T(\mathbf{e}_n) \end{bmatrix}]. Now we should show that this stem:[T(\mathbf{v}) = \mathbf{w}^\top \mathbf{v}] holds for all stem:[\mathbf{v} \in \mathbb{V}].

[stem]
++++
\begin{align*}
T(\mathbf{v}) & = T(v_1 \mathbf{e}_1 + v_2 \mathbf{e}_2 + \dots + v_n \mathbf{e}_n) \\
& = v_1 T(\mathbf{e}_1) + v_2 T(\mathbf{e}_2) + \dots + v_n T(\mathbf{e}_n) && \text{as T is linear}\\
& = v_1 w_1 + v_2 w_2 + \dots + v_n w_n = \mathbf{w}^\top \mathbf{v} && \forall \mathbf{v} \in \mathbb{V}
\end{align*}
++++

So any linear transformation stem:[\mathbb{R}^n \to \mathbb{R}] can be represented by stem:[T(\mathbf{v}) = \mathbf{A}_{1 \times n} \mathbf{v}] and the entries of stem:[\mathbf{A}] can be found by evaluating the transformation at the basis vectors.

image::.\images\linear_trans_02.png[align='center']

We can use any basis (not necessarily standard basis or orthogonal) to define the transformation. Say we are given a basis of stem:[\mathbb{R}^n] as stem:[\{\mathbf{b}_1, \dots, \mathbf{b}_n \}]. Then it is enough to evaluate the transformation at each basis vectors, stem:[T(\mathbf{b}_1), \dots, T(\mathbf{b}_n)]. Then

[stem]
++++
T(\mathbf{v}) = T\left(\sum_{i=1}^n \alpha_i \mathbf{b}_i \right) = \sum_{i=1}^n \alpha_i T(\mathbf{b}_i)
++++

See for the detailed example in the OneNote.

=== To stem:[m] Dimension ===
Let stem:[T: \mathbb{R}^n \rightarrow \mathbb{R}^m] and it is linear. We need a matrix stem:[\mathbf{A}_{m \times n}] to represent the transformation stem:[T(\mathbf{v}) = \mathbf{A} \mathbf{v}].

If the formula holds for all stem:[\mathbf{v} \in \mathbb{R}^n], then it should also hold for basis vectors of stem:[\mathbb{R}^n]. Let stem:[\{\mathbf{e}_1, \dots, \mathbf{e}_n \}] be the standard basis of stem:[\mathbb{R}^n].

[stem]
++++
\begin{align*}
T(\mathbf{e}_1) = T\left( \begin{bmatrix} 1 \\ 0 \\ \vdots \\ 0 \end{bmatrix} \right) & = \mathbf{A} \begin{bmatrix} 1 \\ 0 \\ \vdots \\ 0 \end{bmatrix} = \text{First column of } \mathbf{A} \\
\dots \\
T(\mathbf{e}_i) = T\left( \begin{bmatrix} 0 \\ 1 \\ \vdots \\ 0 \end{bmatrix} \right) & = \mathbf{A} \begin{bmatrix} 0 \\ 1 \\ \vdots \\ 0 \end{bmatrix} = i^{th} \text{column of } \mathbf{A} \text{ for } i=2,\dots,n \\
\end{align*}
++++

Thus we get the matrix stem:[\mathbf{A}]

[stem]
++++
\mathbf{A} = \begin{bmatrix} \vdots & \vdots & \vdots \\ T(\mathbf{e}_1) & \dots & T(\mathbf{e}_n) \\ \vdots & \vdots & \vdots \end{bmatrix}_{m \times n}
++++

Then for any vector stem:[\mathbf{v} \in \mathbb{R}^n]

[stem]
++++
\begin{align*}
T(\mathbf{v}) & = T(v_1 \mathbf{e}_1 + v_2 \mathbf{e}_2 + \dots + v_n \mathbf{e}_n) \\
& = v_1 T(\mathbf{e}_1) + v_2 T(\mathbf{e}_2) + \dots + v_n T(\mathbf{e}_n) && \text{as T is linear}\\
& = \mathbf{A} \mathbf{v} && \forall \mathbf{v} \in \mathbb{R}^n
\end{align*}
++++

This transformation is with respect to the standard basis of stem:[\mathbb{R}^n] and the standard basis of stem:[\mathbb{R}^m], i.e., the input vector is given with respect to the standard basis of stem:[\mathbb{R}^n] and the output vector will be with respect to the standard basis of stem:[\mathbb{R}^m].